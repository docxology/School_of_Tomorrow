{
  "id": "7234426471687826281",
  "title": "More Echo Chambers",
  "published": "2023-01-12T20:52:00.000-08:00",
  "updated": "2023-01-14T22:07:18.042-08:00",
  "content": "[](https://www.flickr.com/photos/kirbyurner/52545191527/in/dateposted-public/)\n\nWe should revisit the image of \"echo chamber\", which I think is a good one, yet even more powerful if paired with other words such as \"groupthink\" and \"choir\", even \"silo\".\u00a0 I'd make a great machine learner, or am one already, as Chelsea Manning suggested in Berlin that time.\n\nYes, I've been exploring the world (planet) of NLP, natural language processing.\u00a0 The specialists have carved out a nifty niche, with a smooth vocab, kudos to University of Toronto.\u00a0\u00a0\n\nWhen I tell the story, I let Regular Polytopes hitch a ride on the ML bandwagon, because Graph Theory, because Linear Algebra.\u00a0 I'm linking Regular Polytopes to Coxeter of course, also University of Toronto, and practitioner of hyper-dimensional Euclidean language games.\n\nDid you know Coxeter lent his suite at Cambridge to the Wittgenstein posse, even after deciding what passed for hot philo was not his cup of tea.\u00a0 Ludwig (LW) had entered his \"second phase\" by then (\"language games\"), if I'm not mistaken.\u00a0 My source is the Siobhan Roberts Donald Coxeter bio:\u00a0 The King of Infinite Space.\u00a0 She did a later bio of Conway.\n\nI'm not the expert in the room when it comes to NLP.\u00a0 I do rub up against Machine Learning in my contemporary role on faculty, but more the way a high school teacher rubs against higher math, as an admirer and fan.\u00a0 I'll lurk in on Andrius talking Sheffer Polynomials and even get deeper into his slides, but I'm not venturing as an equal into the ring.\u00a0 I'm an apprentice at best in so many arenas.\n\nMy understanding is a breakthrough in the concepts of Attention and Self Attention (operationally defined) has enabled \"next word predictors\" to take more context into account, in addition to word order.\u00a0 Would it be right to say a weighted semantic network develops internally, connecting higher attention words to their most frequent associates?\u00a0 \"Michelin\" goes with \"guide\" \"travel\" and \"tourist\" as well as \"tire\".\n\nSpeaking of tourism and travel, at one point I was told, while gazing over the maze-like pattern of walls and chambers in the Colosseum in Rome, that I was seeing the ruins of a medieval castle that occupied the Roman stadium in a later century.\u00a0 I believed that, but was recently talked into accepting these walls and chambers date to Roman times, while the castle story is also true (they're not mutually exclusive).\u00a0\n\nActually, my family, mother in particular, knows more about that castle than most, having taken an interest in Jacopa Frangipani, a luminary protagonist in her The Lions and The Lamb, an historical fiction novel, unpublished.\u00a0 The Frangipanis owned the castle.\u00a0 Jacopa was a friend and admirer of St. Francis.\n\nYou may have picked up on what I'm doing here:\u00a0 showing off what \"real intelligence\" (vs the phony stuff) is still capable of, in contrast.\u00a0 Everything may appear strung together haphazardly, but on deeper digging, everything checks out.\u00a0 Eat your heart out GPT-3.",
  "categories": [],
  "author": "Kirby Urner",
  "blog_name": "control_room"
}